{"meta":{"title":"Adopted Irelia","subtitle":"=w=","description":"homepage test","author":"Adopted Irelia","url":"http://example.com","root":"/"},"pages":[{"title":"test","date":"2023-04-19T09:46:03.000Z","updated":"2023-04-20T12:21:52.953Z","comments":true,"path":"test/index.html","permalink":"http://example.com/test/index.html","excerpt":"","text":"haha"},{"title":"friends","date":"2023-04-20T12:31:13.000Z","updated":"2023-04-20T12:31:13.910Z","comments":true,"path":"friends/index.html","permalink":"http://example.com/friends/index.html","excerpt":"","text":""}],"posts":[{"title":"first blog","slug":"first-blog","date":"2023-04-20T00:12:21.000Z","updated":"2023-04-20T00:13:27.091Z","comments":true,"path":"2023/04/20/first-blog/","link":"","permalink":"http://example.com/2023/04/20/first-blog/","excerpt":"","text":"hello","categories":[],"tags":[{"name":"test","slug":"test","permalink":"http://example.com/tags/test/"}]},{"title":"Intermediate Prototype Mining Transformer for Few-Shot Semantic Segmentation","slug":"2023-04-20-Intermediate Prototype Mining Transformer for Few-Shot Semantic Segmentation","date":"2023-04-19T16:00:00.000Z","updated":"2023-04-23T13:51:47.199Z","comments":true,"path":"2023/04/20/2023-04-20-Intermediate Prototype Mining Transformer for Few-Shot Semantic Segmentation/","link":"","permalink":"http://example.com/2023/04/20/2023-04-20-Intermediate%20Prototype%20Mining%20Transformer%20for%20Few-Shot%20Semantic%20Segmentation/","excerpt":"","text":"Abstract Most previous works strive to mine more effective category information from the support to match with the corresponding objects in query. However, they all ignored the category information gap between query and support images. we are the first to introduce an intermediate prototype for mining both deterministic category information from the support and adaptive category knowledge from the query Motivation However, for the support images that have large diversity in pose and appearance compared with the query, the distance between the support and query prototypes will be faraway. In such a case, if we forcibly migrate the category information in the support prototype to the query, a large category information bias is inevitably introduced. Contribution To the best of our knowledge, this is the first time to focus on the intra-class diversity between support and query in FSS, and we propose the idea of intermediate prototype to relieve the existing category information gap issue. We propose a novel IPMT to explicitly mine the intermediate prototype which contains both the deterministic information from the support set and the adaptive category knowledge from the query We present an iterative learning scheme to fully explore the intermediate category information hidden in both support and query and update the query feature. Extensive experiments on PASCAL-5i and COCO-20i show that our proposed IPMT brings a significant improvement over state-of-the-art methods. Method Intermediate Prototype Mining Conclusion","categories":[{"name":"study paper-reading","slug":"study-paper-reading","permalink":"http://example.com/categories/study-paper-reading/"}],"tags":[{"name":"Few-Shot Semantic-Segmentation","slug":"Few-Shot-Semantic-Segmentation","permalink":"http://example.com/tags/Few-Shot-Semantic-Segmentation/"}]},{"title":"CATrans:Context and Affinity Transformer for Few-Shot Segmentation","slug":"2023-04-21-CATrans: Context and Affinity Transformer for Few-Shot Segmentation","date":"2023-04-19T16:00:00.000Z","updated":"2023-02-15T16:00:00.000Z","comments":true,"path":"2023/04/20/2023-04-21-CATrans: Context and Affinity Transformer for Few-Shot Segmentation/","link":"","permalink":"http://example.com/2023/04/20/2023-04-21-CATrans:%20Context%20and%20Affinity%20Transformer%20for%20Few-Shot%20Segmentation/","excerpt":"","text":"Abstract previous Transformer-based methods explore global consensus either on context similarity or affinity map between support-query pairs we effectively integrate the context and affinity information via the proposed novel Context and Affinity Transformer (CATrans) in a hierarchical architecture. Motivation (context)This method, however, suffers from unrepresentative of support feature (affinity)However, this method does not incorporate individual self-affinity for support object or query image to disambiguate noisy correlations, which measures pixel-wise correspondences within itself, enabling each spatial fiber to match itself and other tokens. Contribution We design a Relation-guided Context Transformer (RCT) with the enhanced support features to propagate informative semantic information from support to query images. We develop a Relation-guided Affinity Transformer (RAT) to measure the reliable cross correspondences by considering the auxiliary self-affinity of both support object and query images. We propose Context and Affinity Transformer, dubbed as CATrans, in a hierarchical architecture to aggregate the context and affinity together, resulting in discriminative representations from support to query mask, enhancing robustness to intra-class variations between support and query images. Our CATrans outperforms the state-of-the-art methods on two benchmarks, Pascal-5i and COCO- 20i. Method Intermediate Prototype Mining Conclusion","categories":[{"name":"study paper-reading","slug":"study-paper-reading","permalink":"http://example.com/categories/study-paper-reading/"}],"tags":[{"name":"Few-Shot Semantic-Segmentation","slug":"Few-Shot-Semantic-Segmentation","permalink":"http://example.com/tags/Few-Shot-Semantic-Segmentation/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-04-19T09:03:10.843Z","updated":"2023-04-19T09:03:10.843Z","comments":true,"path":"2023/04/19/hello-world/","link":"","permalink":"http://example.com/2023/04/19/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Few-Shot Segmentation via Cycle-Consistent Transformer","slug":"2023-03-08-Few-Shot Segmentation via Cycle-Consistent Transformer","date":"2023-03-07T16:00:00.000Z","updated":"2023-04-23T13:51:47.194Z","comments":true,"path":"2023/03/08/2023-03-08-Few-Shot Segmentation via Cycle-Consistent Transformer/","link":"","permalink":"http://example.com/2023/03/08/2023-03-08-Few-Shot%20Segmentation%20via%20Cycle-Consistent%20Transformer/","excerpt":"","text":"Abstract In this paper, we focus on utilizing pixel-wise relationships between support and query images to facilitate the few-shot semantic segmentation. Directly performing cross-attention may aggregate these features from support to query and bias the query features. Thus, we propose using a novel cycle-consistent attention mechanism to filter out possible harmful support feature and encourage query features to attend to the most informative pixels from support images. Motivation class-wise mean pooling (a). support features within regions of different categories are averaged to serve as prototypes to facilitate the classification of query pixels. clustering (b). recent works attempt to generate multiple prototypes via EM algorithm or K-means clustering, in order to extract more abundant information from support images. these prototype-based methods need to “compress” support information into different prototypes, which may lead to various degrees of loss of beneficial support information. © propose to employ the attention mechanism to extract information from foreground. However, such methods ignore background support pixels that can be beneficial for segmenting query images. Contribution We tackle few-shot semantic segmentation from the perspective of providing each query pixel with relevant information from support images through pixel-wise alignment We propose a novel cycle-consistent transformer to aggregate the pixel-wise support features into the query ones. In CyCTR, we observe that many support features may confuse the attention ans bias pixel-level feature aggregation, and propose incorporating cycle-consistent operation into the attention to deal with this issue. Our CyCTR achieves state of art. Method Cycle-Consistent Transformer Cycle-Consistent Attention this module is used to identify cycle-consistent. first, find the most similar i from j. i∗=arg⁡max⁡θA(i,j)i^* = \\mathop{\\arg\\max}\\limits_{\\theta} A_{(i,j)} i∗=θargmax​A(i,j)​ then, cycle back. j∗=arg⁡max⁡θA(i∗,j)j^* = \\mathop{\\arg\\max}\\limits_{\\theta} A_{(i^*,j)} j∗=θargmax​A(i∗,j)​ if Ms(j)=Ms(j∗)M_{s(j)} = M_{s(j^*)}Ms(j)​=Ms(j∗)​, we accept the cycle-consistent. Bj={0,ifMs(j)=Ms(j∗)−inf,ifMs(j)≠Ms(j∗)B_j = \\begin{cases} 0, if M_{s(j)} = M_{s(j^*)} \\\\ -inf, if M_{s(j)} \\neq M_{s(j^*)} \\end{cases} Bj​={0,ifMs(j)​=Ms(j∗)​−inf,ifMs(j)​=Ms(j∗)​​ so the final attention is Cycleatt(Q,K,V)=softmax(Ai+B)VCycleatt(Q,K,V) = softmax(A_i + B)V Cycleatt(Q,K,V)=softmax(Ai​+B)V it means when B is not cycle-consistent, then we pay no attention to B. Conclusion our CyCTR utilizes all pixel-level support features and can effectively eliminate aggregating confusing and harmful support features with the proposed novel cycle-consistency attention","categories":[{"name":"study paper-reading","slug":"study-paper-reading","permalink":"http://example.com/categories/study-paper-reading/"}],"tags":[{"name":"Few-Shot Semantic-Segmentation","slug":"Few-Shot-Semantic-Segmentation","permalink":"http://example.com/tags/Few-Shot-Semantic-Segmentation/"}]},{"title":"Remember the Difference:Cross-Domain Few-Shot Semantic Segmentation via Meta-Memory Transfer","slug":"2023-03-02-Remeber","date":"2023-03-01T16:00:00.000Z","updated":"2023-04-23T13:51:47.188Z","comments":true,"path":"2023/03/02/2023-03-02-Remeber/","link":"","permalink":"http://example.com/2023/03/02/2023-03-02-Remeber/","excerpt":"","text":"original paper: link Abstract we propose an interesting and challenging cross-domain few-shot semantic segmentation task, where the training and test tasks perform on different domains. we propose a meta-memory bank to improve the generalization of the segmentation network by bridging the domain gap between source and target domains we adopt a new contrastive learning strategy to explore the knowledge of different categories during the training stage. Motivation Existing approaches assume that the base training set is sampled from the same domain as the testing set. However, this setting is not always guaranteed. Contribution We propose a novel framework to solve the cross-domain problem in few-shot semantic segmentation. Compared to the standard few-shot segmentation network, we use the most primitive feature transfer to solve the cross-domain problem and effectively broaden the use scenarios of few-shot segmentation tasks. We propose a plug-and-play meta-knowledge module to transfer the prior source distribution to the target do-main. Our model can effectively alleviate the influence of domain shift in few-shot segmentation with exclusive contrastive loss. We demonstrate the effectiveness of our framework on four different cross-domain few-shot segmentation scenarios. In particular, it can achieve state-of-the-art performance under the cross-domain setting. Method Meta-Memory Module function: storing the source data distribution; using meta-knowledge for feature enhancement Memory={M=(mj∈R1×C)j=1N,E=(ej∈R1×C)j=1N}Memory = \\left\\{M = (m_j \\in R^{1 \\times C})_{j=1}^N, E = (e_j \\in R^{1 \\times C})_{j=1}^N \\right\\} Memory={M=(mj​∈R1×C)j=1N​,E=(ej​∈R1×C)j=1N​} for (a): first get input of features of pictures: fb=RC×H×Wf_b = R^{C \\times H \\times W} fb​=RC×H×W then,calculate the mean and variance: μb=1HW∑h=1H∑w=1Wfc,h,w\\mu_b = \\frac{1}{HW}\\sum^H_{h=1}\\sum^W_{w=1}f_{c,h,w} μb​=HW1​h=1∑H​w=1∑W​fc,h,w​ vb=1HW∑h=1H∑w=1W(fc,h,w−μb)2v_b = \\sqrt{\\frac{1}{HW}\\sum^H_{h=1}\\sum^W_{w=1}(f_{c,h,w} - \\mu_b)^2} vb​=HW1​h=1∑H​w=1∑W​(fc,h,w​−μb​)2​ then, normalize: fbform=fb−μbvbf_b^{form} = \\frac{f_b - \\mu_b}{v_b} fbform​=vb​fb​−μb​​ then, calculate the similarity: sMjb=sim(mj,μb)∑b=1Bsim(mj,μb)s_M^{jb} = \\frac{sim(m_j,\\mu_b)}{\\sum^B_{b=1}sim(m_j,\\mu_b)} sMjb​=∑b=1B​sim(mj​,μb​)sim(mj​,μb​)​ sEjb=sim(ej,vb)∑b=1Bsim(ej,vb)s_E^{jb} = \\frac{sim(e_j,v_b)}{\\sum^B_{b=1}sim(e_j,v_b)} sEjb​=∑b=1B​sim(ej​,vb​)sim(ej​,vb​)​ finally, update the Memory: mj=λmj+(1−λ)∑b=1BsMjbμbm_j = \\lambda m_j + (1-\\lambda)\\sum^B_{b=1} s_M^{jb} \\mu_b mj​=λmj​+(1−λ)b=1∑B​sMjb​μb​ ej=λej+(1−λ)∑b=1BsMjbvbe_j = \\lambda e_j + (1-\\lambda)\\sum^B_{b=1} s_M^{jb} v_b ej​=λej​+(1−λ)b=1∑B​sMjb​vb​ the loss is for lower the similarity of each metas: Lorth=12N2(∑i=1N∑j=1NhMij+∑i=1N∑j=1NhEij)L_{orth} = \\frac{1}{2N^2}(\\sum^N_{i=1}\\sum^N_{j=1}h_M^{ij} + \\sum^N_{i=1}\\sum^N_{j=1}h_E^{ij}) Lorth​=2N21​(i=1∑N​j=1∑N​hMij​+i=1∑N​j=1∑N​hEij​) for (b) source data enhance enhanced feature: fbenh=fbnormvbmix+μbmixf_b^{enh} = f_b^{norm} v_b^{mix} + \\mu_b^{mix} fbenh​=fbnorm​vbmix​+μbmix​ where mix are: μbmix=αμb+(1−α)μbM\\mu_b^{mix} = \\alpha \\mu_b + (1-\\alpha)\\mu_b^M μbmix​=αμb​+(1−α)μbM​ vbmix=αvb+(1−α)vbEv_b^{mix} = \\alpha v_b + (1-\\alpha)v_b^E vbmix​=αvb​+(1−α)vbE​ μbM and vbE\\mu_b^M \\ and\\ v_b^E μbM​ and vbE​ are the lowest similarity. target data enhance target data enhance is similar to source data enhance. the difference is $ \\mu_b^M \\ and\\ v_b^E$ are chosen to be max similarity. loss of prototype loss is : Lpro=−1HW∑i=1H∑j=1WYqijlog(R(fqij,pb))L_{pro} = -\\frac{1}{HW}\\sum^H_{i=1}\\sum^W_{j=1} Y_q^{ij}log(R(f_q^{ij},p_b)) Lpro​=−HW1​i=1∑H​j=1∑W​Yqij​log(R(fqij​,pb​)) contrastive loss: Lcont=12B∑i=1B−logpos(i)pos(i)+net(i)L_{cont} = \\frac{1}{2B} \\sum^B_{i=1} -log\\frac{pos(i)}{pos(i) + net(i)} Lcont​=2B1​i=1∑B​−logpos(i)+net(i)pos(i)​ where pos(i)=exp(sim(pbi,pti))pos(i) = exp(sim(p_b^i,p_t^i)) pos(i)=exp(sim(pbi​,pti​)) neg(i)=∑j=1,j≠iB[exp(sim(pbi,pbj))+exp(sim(pbi,ptj))] neg(i) = \\sum^B_{j=1,j \\neq i} [exp(sim(p_b^i,p_b^j))+exp(sim(p_b^i,p_t^j))] neg(i)=j=1,j=i∑B​[exp(sim(pbi​,pbj​))+exp(sim(pbi​,ptj​))] Conclusion a meta-memory module has been proposed to bridge the source and target domains, including reducing the domain gap and enhancing semantic feature representation. Specifically, the meta-memory stores the domain-specific information from the source data during training and transfers them to the target data to improve the generalization of the segmentation model. The memory-based feature enhancement also contributes to discriminative feature learning for the novel classes.","categories":[{"name":"study paper-reading","slug":"study-paper-reading","permalink":"http://example.com/categories/study-paper-reading/"}],"tags":[{"name":"Few-Shot Semantic-Segmentation","slug":"Few-Shot-Semantic-Segmentation","permalink":"http://example.com/tags/Few-Shot-Semantic-Segmentation/"}]},{"title":"Mask Matching Transformer for Few-Shot Segmentation","slug":"2023-02-24-Mask Matching Transformer for Few-Shot Segmentation","date":"2023-02-25T16:00:00.000Z","updated":"2023-04-24T03:07:58.832Z","comments":true,"path":"2023/02/26/2023-02-24-Mask Matching Transformer for Few-Shot Segmentation/","link":"","permalink":"http://example.com/2023/02/26/2023-02-24-Mask%20Matching%20Transformer%20for%20Few-Shot%20Segmentation/","excerpt":"","text":"original paper: link Abstract Typical method learn prototypical features to match query features. They proposed Mask Matching Transformer(MM-Former), a new paradigm. Motivation early method: few to many(the number of support prototypes is typically much less than query features) other method: many to many Overall, the aforementioned approaches construct modules combining the matching operation with segmentation modules and optimizing them jointly. This joint learning fashion not only vastly increases the learning complexity, but also makes it hard to distinguish the effect of matching modules in few-shot segmentation. Contribution We put forward a new perspective for few-shot segmentation, which decouples the learning of matching and segmentation modules, allowing more flexibility and lower training complexity. We introduce a simple two-stage framework named MM-Former that efficiently matches the support samples with a set of query mask proposals to obtain segmentation results. Extensive evaluations on COCO-20i and Pascal-5i demonstrate the potential of the method to be a robust baseline in the few-to-few matching paradigm Method Potential Object Segmenter El+1=TLayerl(El,Fi)E^{l+1} = TLayer^{l}(E^l,F_i) El+1=TLayerl(El,Fi​) ElE^lEl represent the N learnable embeddings before transformer, while El+1E^{l+1}El+1 represents the embeddings after transformer. TLayer denotes a transformer decoder layer. Feature Align Module Learnable Matching Block Final mask is computed as: S=cos(Psgt,PQn)S = cos(P_s^{gt},P_Q^n)S=cos(Psgt​,PQn​) M^=M matmul MLP(S)\\hat{M} = M\\ matmul \\ MLP(S)M^=M matmul MLP(S) Conclusion Our MM-Former introduces the paradigm of decompose first and then blend to the research of few-shot segmentation, which is a totally new perspective and may inspire future researchers to develop more advanced versions. However, there is still a large gap between the current results and the oracle (≈ 20% mIoU). How to further narrow this gap is our future research focus.","categories":[{"name":"study","slug":"study","permalink":"http://example.com/categories/study/"},{"name":"paper-reading","slug":"study/paper-reading","permalink":"http://example.com/categories/study/paper-reading/"}],"tags":[{"name":"Few-Shot","slug":"Few-Shot","permalink":"http://example.com/tags/Few-Shot/"},{"name":"Semantic-Segmentation","slug":"Semantic-Segmentation","permalink":"http://example.com/tags/Semantic-Segmentation/"}]}],"categories":[{"name":"study paper-reading","slug":"study-paper-reading","permalink":"http://example.com/categories/study-paper-reading/"},{"name":"study","slug":"study","permalink":"http://example.com/categories/study/"},{"name":"paper-reading","slug":"study/paper-reading","permalink":"http://example.com/categories/study/paper-reading/"}],"tags":[{"name":"test","slug":"test","permalink":"http://example.com/tags/test/"},{"name":"Few-Shot Semantic-Segmentation","slug":"Few-Shot-Semantic-Segmentation","permalink":"http://example.com/tags/Few-Shot-Semantic-Segmentation/"},{"name":"Few-Shot","slug":"Few-Shot","permalink":"http://example.com/tags/Few-Shot/"},{"name":"Semantic-Segmentation","slug":"Semantic-Segmentation","permalink":"http://example.com/tags/Semantic-Segmentation/"}]}