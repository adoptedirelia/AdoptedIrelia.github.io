---
title: Remember the Difference:Cross-Domain Few-Shot Semantic Segmentation via Meta-Memory Transfer
commentable: true
date: 2023-03-02
modified: false
Edit: 2023-02-16
mathjax: true
mermaid: true
tags: Few-Shot Semantic-Segmentation
categories: study paper-reading 
description: (2022CVPR)Remember the Difference:Cross-Domain Few-Shot Semantic Segmentation via Meta-Memory Transfer
---

original paper: [link](https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Remember_the_Difference_Cross-Domain_Few-Shot_Semantic_Segmentation_via_Meta-Memory_Transfer_CVPR_2022_paper.html)

# Abstract 

- we propose an interesting and challenging cross-domain few-shot semantic segmentation task, where the training and test tasks perform on different domains.
- we propose  a meta-memory bank to improve the generalization of the segmentation network by bridging the domain gap between source and target domains
-  we adopt a new contrastive learning strategy to explore the knowledge of different categories during the training stage.


# Motivation

Existing approaches assume that the base training set is sampled from the same domain as the testing set.

However, this setting is not always guaranteed.

# Contribution

- We propose a novel framework to solve the cross-domain problem in few-shot semantic segmentation. Compared to the standard few-shot segmentation network, we use the most primitive feature transfer to solve the cross-domain problem and effectively broaden the use scenarios of few-shot segmentation tasks.

- We propose a plug-and-play meta-knowledge module to transfer the prior source distribution to the target do-main. Our model can effectively alleviate the influence of domain shift in few-shot segmentation with exclusive contrastive loss.

- We demonstrate the effectiveness of our framework on four different cross-domain few-shot segmentation scenarios. In particular, it can achieve state-of-the-art performance under the cross-domain setting.

# Method
<img src="https://raw.githubusercontent.com/adoptedirelia/pictures_of_posts/main/Remember%20the%20Difference%3ACross-Domain%20Few-Shot%20Semantic%20Segmentation%20via%20Meta-Memory%20Transfer/pic2.png" width="100%"> 
## Meta-Memory Module

function: 

1. storing the source data distribution; 
2. using meta-knowledge for feature enhancement

<img src="https://raw.githubusercontent.com/adoptedirelia/pictures_of_posts/main/Remember%20the%20Difference%3ACross-Domain%20Few-Shot%20Semantic%20Segmentation%20via%20Meta-Memory%20Transfer/pic1.png" width="50%"> 

$ Memory = \left\{M = (m_j \in R^{1 \times C})_{j=1}^N, E =  (e_j \in R^{1 \times C})_{j=1}^N \right\} $


### for (a):


first get input of features of pictures:

$ f_b = R^{C \times H \times W} $

then,calculate the mean and variance:

$ \mu_b = \frac{1}{HW}\sum^H_{h=1}\sum^W_{w=1}f_{c,h,w} $

$ v_b = \sqrt{\frac{1}{HW}\sum^H_{h=1}\sum^W_{w=1}(f_{c,h,w} - \mu_b)^2} $

then, normalize:

$ f_b^{form} = \frac{f_b - \mu_b}{v_b} $

then, calculate the similarity:

$ s_M^{jb} = \frac{sim(m_j,\mu_b)}{\sum^B_{b=1}sim(m_j,\mu_b)} $

$ s_E^{jb} = \frac{sim(e_j,v_b)}{\sum^B_{b=1}sim(e_j,v_b)} $

finally, update the Memory:

$ m_j = \lambda m_j + (1-\lambda)\sum^B_{b=1} s_M^{jb} \mu_b $

$ e_j =  \lambda e_j + (1-\lambda)\sum^B_{b=1} s_M^{jb} v_b $


the loss is for lower the similarity of each metas:

$ L_{orth} = \frac{1}{2N^2}(\sum^N_{i=1}\sum^N_{j=1}h_M^{ij} + \sum^N_{i=1}\sum^N_{j=1}h_E^{ij}) $

### for (b)

#### source data enhance

enhanced feature:

$ f_b^{enh} = f_b^{norm} v_b^{mix} + \mu_b^{mix} $


where mix are:

$ \mu_b^{mix}  = \alpha \mu_b + (1-\alpha)\mu_b^M $

$ v_b^{mix}  = \alpha v_b + (1-\alpha)v_b^E $

$ \mu_b^M \ and\  v_b^E $ 

are the lowest similarity.

#### target data enhance 

target data enhance is similar to source data enhance. 

the difference is $ \mu_b^M \ and\  v_b^E$ are chosen to be max similarity. 

loss of prototype loss is :

$ L_{pro} = -\frac{1}{HW}\sum^H_{i=1}\sum^W_{j=1} Y_q^{ij}log(R(f_q^{ij},p_b)) $

contrastive loss:

$ L_{cont} = \frac{1}{2B} \sum^B_{i=1} -log\frac{pos(i)}{pos(i) + net(i)} $

where 

$ pos(i) = exp(sim(p_b^i,p_t^i))  $

$ neg(i)  = \sum^B_{j=1,j \neq i} [exp(sim(p_b^i,p_b^j))+exp(sim(p_b^i,p_t^j))] $


# Conclusion

-  a meta-memory module has been proposed to bridge the source and
target domains, including reducing the domain gap and enhancing semantic feature representation.

- Specifically, the
meta-memory stores the domain-specific information from
the source data during training and transfers them to the
target data to improve the generalization of the segmentation model. The memory-based feature enhancement also
contributes to discriminative feature learning for the novel
classes.